{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b00291b4-26e4-4613-99b1-ee0ca6e1ba3a",
   "metadata": {},
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f911b6e5-268c-411c-bf7f-6b0b3e97f917",
   "metadata": {},
   "source": [
    "In machine learning algorithms, particularly in the context of Support Vector Machines (SVMs), the relationship between polynomial functions and kernel functions is significant. Here's an overview:\n",
    "\n",
    "### Polynomial Functions:\n",
    "- Polynomial functions are mathematical functions that involve variables raised to powers and multiplied by coefficients. They are commonly used to model relationships between variables in various fields, including mathematics, physics, and engineering.\n",
    "- In SVMs, polynomial functions can be used as kernel functions to map input data into higher-dimensional feature spaces, where linear separation may be possible even when the original data is not linearly separable in the input space.\n",
    "- The polynomial kernel function is defined as \\( K(\\mathbf{x}, \\mathbf{y}) = (\\mathbf{x}^T \\mathbf{y} + c)^d \\), where \\( \\mathbf{x} \\) and \\( \\mathbf{y} \\) are input vectors, \\( c \\) is a constant term, and \\( d \\) is the degree of the polynomial.\n",
    "\n",
    "### Kernel Functions:\n",
    "- Kernel functions are mathematical functions that compute the similarity or dot product between pairs of data points in a feature space. They allow SVMs to operate in high-dimensional feature spaces without explicitly computing the transformations.\n",
    "- In SVMs, kernel functions play a crucial role in the kernel trick, which enables efficient computation of decision boundaries in high-dimensional spaces without explicitly computing the transformed feature vectors.\n",
    "- Polynomial kernel functions are a specific type of kernel function used in SVMs, where the dot product between pairs of data points is computed using polynomial functions.\n",
    "\n",
    "### Relationship:\n",
    "- Polynomial functions can be used as kernel functions in SVMs to implicitly map input data into higher-dimensional feature spaces, enabling nonlinear decision boundaries to be learned in the original input space.\n",
    "- The polynomial kernel function calculates the dot product between pairs of data points using polynomial functions, allowing SVMs to capture nonlinear relationships in the data without explicitly computing the transformations.\n",
    "- In essence, polynomial functions serve as the basis for polynomial kernel functions, which are a type of kernel function used in SVMs to model nonlinear relationships between data points.\n",
    "\n",
    "In summary, polynomial functions and kernel functions, particularly polynomial kernel functions in the context of SVMs, are closely related as polynomial functions can be used as kernel functions to enable nonlinear classification in high-dimensional feature spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fbdaa9-bae3-4634-bb56-2e0eebb968e6",
   "metadata": {},
   "source": [
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96f5b16-8d8e-461f-be02-a66f3c7caeeb",
   "metadata": {},
   "source": [
    "Implementing an SVM with a polynomial kernel in Python using Scikit-learn is straightforward. You can follow these steps:\n",
    "\n",
    "Import the necessary libraries:\n",
    "\n",
    "We'll need Scikit-learn's SVC class to implement the SVM with a polynomial kernel.\n",
    "Load or generate your dataset:\n",
    "\n",
    "Prepare your dataset with features (X) and corresponding labels (y).\n",
    "Split the dataset into training and testing sets:\n",
    "\n",
    "Divide your dataset into a training set and a testing set to evaluate the performance of the SVM.\n",
    "Instantiate the SVM model:\n",
    "\n",
    "Create an instance of the SVC class and specify the kernel parameter as 'poly' to indicate that you want to use a polynomial kernel.\n",
    "Train the SVM model:\n",
    "\n",
    "Fit the SVM model to the training data using the fit method.\n",
    "Make predictions:\n",
    "\n",
    "Use the trained model to make predictions on the testing data using the predict method.\n",
    "Evaluate the model:\n",
    "\n",
    "Assess the performance of the SVM model by comparing the predicted labels with the actual labels from the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b11a58a-19c4-41a8-9b6e-ef9f2dfe9da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM with polynomial kernel: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Instantiate the SVM model with a polynomial kernel\n",
    "svm_poly = SVC(kernel='poly', degree=3)  # You can specify the degree of the polynomial (default is 3)\n",
    "\n",
    "# Train the SVM model\n",
    "svm_poly.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_poly.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = svm_poly.score(X_test, y_test)\n",
    "print(\"Accuracy of SVM with polynomial kernel:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413c603b-cf08-40f7-a03d-ce9b282ee854",
   "metadata": {},
   "source": [
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21767b09-a8e5-47d0-abd7-9c42d8e4d1fc",
   "metadata": {},
   "source": [
    "In Support Vector Regression (SVR), epsilon (ε) is a hyperparameter that controls the width of the margin around the regression line within which no penalty is incurred. It defines the tube around the regression line where data points are considered to be accurately predicted and do not contribute to the loss function.\n",
    "\n",
    "Here's how increasing the value of epsilon affects the number of support vectors in SVR:\n",
    "\n",
    "1. **Decreasing Epsilon**:\n",
    "   - When the value of epsilon is small, the tube around the regression line is narrow. This means that the SVR model is more sensitive to deviations of data points from the regression line.\n",
    "   - As a result, more data points may fall outside the tube, leading to a larger number of support vectors. These support vectors are the data points that lie on the margin boundary or within the margin, influencing the position and orientation of the regression line.\n",
    "   - With a narrow tube, the model may be prone to overfitting, capturing noise in the data, and resulting in a complex model.\n",
    "\n",
    "2. **Increasing Epsilon**:\n",
    "   - When the value of epsilon is large, the tube around the regression line is wider. This means that the SVR model is more tolerant of deviations of data points from the regression line.\n",
    "   - As a result, fewer data points fall outside the wider tube, leading to a smaller number of support vectors. The model focuses on capturing the general trend of the data rather than fitting individual data points closely.\n",
    "   - With a wider tube, the model is less prone to overfitting and may generalize better to unseen data. However, it may also result in a simpler model that might not capture all the nuances of the data.\n",
    "\n",
    "In summary, increasing the value of epsilon in SVR tends to decrease the number of support vectors by allowing a wider margin around the regression line, making the model less sensitive to individual data points and potentially reducing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aafd5b1-6ed9-45b3-87ba-2c6c193c9481",
   "metadata": {},
   "source": [
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f09cfe5-3495-48c2-8737-a668c7c3e2c9",
   "metadata": {},
   "source": [
    "Support Vector Regression (SVR) is a powerful regression technique that relies on several hyperparameters to control its behavior and performance. Here's how each parameter affects SVR and when you might want to adjust its value:\n",
    "\n",
    "1. **Kernel Function**:\n",
    "   - The kernel function determines the type of decision boundary used by the SVR model. Common kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid.\n",
    "   - **Effect**: The choice of kernel function affects the flexibility and complexity of the SVR model. A linear kernel may result in a simpler model with a linear decision boundary, while non-linear kernels like RBF can capture more complex relationships in the data.\n",
    "   - **Example**: If the relationship between features and target variable is linear, a linear kernel may suffice. For non-linear relationships, RBF or polynomial kernels might be more suitable.\n",
    "\n",
    "2. **C Parameter**:\n",
    "   - The C parameter controls the trade-off between maximizing the margin and minimizing the training error. A smaller C value allows for a wider margin but may lead to more training errors, while a larger C value penalizes errors more heavily and results in a narrower margin.\n",
    "   - **Effect**: Increasing the C parameter makes the model more sensitive to individual data points, potentially leading to overfitting if set too high. Decreasing C allows for a more flexible margin and can help prevent overfitting but may increase bias.\n",
    "   - **Example**: If the training data contains outliers or noise, increasing C may help the model fit the data better. However, if the data is clean and well-behaved, a smaller C value may suffice to prevent overfitting.\n",
    "\n",
    "3. **Epsilon Parameter (ε)**:\n",
    "   - Epsilon defines the width of the epsilon-insensitive tube around the regression line. Data points within this tube are not considered errors and do not contribute to the loss function.\n",
    "   - **Effect**: Increasing epsilon widens the tolerance for errors, resulting in a wider tube and potentially fewer support vectors. Decreasing epsilon makes the model less tolerant of errors and may lead to a narrower tube and more support vectors.\n",
    "   - **Example**: If the target variable has high variance or noise, increasing epsilon can make the model more robust by ignoring small deviations from the regression line. Conversely, if the data is precise and well-behaved, decreasing epsilon may improve accuracy.\n",
    "\n",
    "4. **Gamma Parameter**:\n",
    "   - Gamma (γ) is a parameter specific to certain kernel functions like RBF. It defines the influence of a single training example, with low values meaning 'far' and high values meaning 'close'.\n",
    "   - **Effect**: Higher gamma values result in a more complex decision boundary, with each data point having a narrower influence range. Lower gamma values result in a smoother decision boundary.\n",
    "   - **Example**: If the dataset is highly non-linear and complex, increasing gamma may help capture intricate patterns. However, too high a gamma can lead to overfitting. For smoother decision boundaries, lower gamma values are preferred.\n",
    "\n",
    "In summary, each parameter in SVR plays a crucial role in determining the model's performance and generalization ability. Understanding how to tune these parameters based on the dataset's characteristics and desired model complexity is essential for building effective SVR models. Regularization parameters like C and epsilon control model flexibility and robustness, while kernel parameters like gamma define the complexity of the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f48fb09-9726-4c94-ac7b-23f5e44fb6b8",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "L Import the necessary libraries and load the dataseg\n",
    "L Split the dataset into training and testing setZ\n",
    "L Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\n",
    "L Create an instance of the SVC classifier and train it on the training datW\n",
    "L hse the trained classifier to predict the labels of the testing datW\n",
    "L Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "precision, recall, F1-scoreK\n",
    "L Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\n",
    "improve its performanc_\n",
    "L Train the tuned classifier on the entire dataseg\n",
    "L Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cca1d135-e238-473c-ba4e-79f77f19a008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n",
      "Best Parameters: {'C': 10, 'gamma': 'scale', 'kernel': 'linear'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tuned_svc_classifier.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svc_classifier = SVC()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svc_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Use the trained classifier to predict labels of the testing data\n",
    "y_pred = svc_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance of the classifier using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Evaluate the performance of the classifier using classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Tune hyperparameters using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': ['scale', 'auto'], 'kernel': ['linear', 'rbf', 'poly']}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "tuned_svc_classifier = SVC(**best_params)\n",
    "tuned_svc_classifier.fit(X, y)\n",
    "\n",
    "# Save the trained classifier to a file\n",
    "joblib.dump(tuned_svc_classifier, 'tuned_svc_classifier.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
