{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97623a3b-548b-4659-a209-e123656f5921",
   "metadata": {},
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2b66df-2537-4dee-b5c1-ccd7984fafc8",
   "metadata": {},
   "source": [
    "To find the probability that an employee is a smoker given that he/she uses the health insurance plan, we can use Bayes' theorem. \n",
    "\n",
    "Let:\n",
    "- \\( S \\) be the event that an employee is a smoker.\n",
    "- \\( H \\) be the event that an employee uses the health insurance plan.\n",
    "\n",
    "We are given:\n",
    "- \\( P(H) = 0.70 \\) (probability that an employee uses the health insurance plan)\n",
    "- \\( P(S|H) = 0.40 \\) (probability that an employee is a smoker given that he/she uses the health insurance plan)\n",
    "\n",
    "We want to find:\n",
    "- \\( P(S|H) \\) (probability that an employee is a smoker given that he/she uses the health insurance plan)\n",
    "\n",
    "Using Bayes' theorem, we have:\n",
    "\n",
    "\\[ P(S|H) = \\frac{P(H|S) \\times P(S)}{P(H)} \\]\n",
    "\n",
    "We are given \\( P(H) \\) and \\( P(S|H) \\), but we need to find \\( P(S) \\) and \\( P(H|S) \\).\n",
    "\n",
    "Since \\( P(H) = 0.70 \\) and \\( P(S|H) = 0.40 \\), we can rearrange Bayes' theorem to find \\( P(S) \\):\n",
    "\n",
    "\\[ P(S) = \\frac{P(H|S) \\times P(S)}{P(H|S)} \\]\n",
    "\n",
    "We can calculate \\( P(H|S) \\) using the fact that \\( P(S|H) = 0.40 \\):\n",
    "\n",
    "\\[ P(H|S) = \\frac{P(S|H) \\times P(H)}{P(S)} = \\frac{0.40 \\times 0.70}{P(S)} \\]\n",
    "\n",
    "Given that \\( P(S) + P(S') = 1 \\), where \\( S' \\) is the complement of \\( S \\) (i.e., the event that an employee is not a smoker), we can rewrite \\( P(S) \\) as \\( 1 - P(S') \\):\n",
    "\n",
    "\\[ P(S) = 1 - P(S') \\]\n",
    "\n",
    "We are also given that \\( P(H') = 0.30 \\), where \\( H' \\) is the event that an employee does not use the health insurance plan. Therefore, \\( P(S'|H') = 1 \\), since all employees who do not use the health insurance plan are not smokers.\n",
    "\n",
    "Now, we can calculate \\( P(S|H) \\):\n",
    "\n",
    "\\[ P(S|H) = \\frac{0.40 \\times 0.70}{0.40 \\times 0.70 + 1 \\times 0.30} \\]\n",
    "\n",
    "\\[ P(S|H) = \\frac{0.28}{0.28 + 0.30} \\]\n",
    "\n",
    "\\[ P(S|H) \\approx \\frac{0.28}{0.58} \\]\n",
    "\n",
    "\\[ P(S|H) \\approx 0.4828 \\]\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is approximately \\( 0.4828 \\) or \\( 48.28\\% \\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96071aac-6481-4b91-9b78-be8ffdd8120a",
   "metadata": {},
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf75cc7-8ed8-492d-bc77-348d7c2a6452",
   "metadata": {},
   "source": [
    "The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the type of features they are designed to handle and the underlying assumptions they make about the data.\n",
    "\n",
    "1. **Feature Representation**:\n",
    "   - **Bernoulli Naive Bayes**: It is suitable for binary feature vectors, where each feature represents the presence or absence of a particular term or attribute. In other words, the features are binary variables.\n",
    "   - **Multinomial Naive Bayes**: It is suitable for discrete feature vectors, typically used for text classification tasks where features represent word counts or term frequencies in documents. Each feature can take on multiple integer values representing the frequency of occurrence of a term.\n",
    "\n",
    "2. **Underlying Assumptions**:\n",
    "   - **Bernoulli Naive Bayes**: It assumes that features are binary variables and independently contribute to the probability of class membership. It is commonly used for text classification tasks where the presence or absence of certain terms in a document is relevant.\n",
    "   - **Multinomial Naive Bayes**: It assumes that features represent counts or frequencies of events occurring in each sample and follow a multinomial distribution. It is commonly used for document classification tasks where the frequency of terms in documents is relevant.\n",
    "\n",
    "3. **Model Parameters**:\n",
    "   - **Bernoulli Naive Bayes**: It typically involves estimating the probabilities of feature presence or absence for each class.\n",
    "   - **Multinomial Naive Bayes**: It typically involves estimating the probabilities of observing each term given the class.\n",
    "\n",
    "4. **Application**:\n",
    "   - **Bernoulli Naive Bayes**: It is commonly used in tasks such as sentiment analysis, spam detection, and document categorization where the presence or absence of certain features is important.\n",
    "   - **Multinomial Naive Bayes**: It is commonly used in text classification tasks such as document categorization, topic modeling, and language identification where the frequency of terms in documents is relevant.\n",
    "\n",
    "In summary, while both Bernoulli and Multinomial Naive Bayes are variants of the Naive Bayes algorithm and are suitable for text classification tasks, they differ in terms of the type of features they handle, their underlying assumptions, and their applications. It is essential to choose the appropriate variant based on the characteristics of the data and the specific task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685c9b92-9ecd-475b-a285-692ff28e759b",
   "metadata": {},
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7825394f-c0e3-4bea-96de-447ab0029d3c",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes handles missing values by considering them as an additional category or state of the feature. When a feature value is missing, it is treated as if the feature is present but its value is not observed. Therefore, in the context of Bernoulli Naive Bayes, missing values are effectively treated as a separate category of the feature.\n",
    "\n",
    "During training, when estimating the probabilities required by the Naive Bayes classifier, the presence of missing values is taken into account along with the observed feature values. The classifier calculates the probabilities of each class given the observed feature values and the probability of the feature being missing.\n",
    "\n",
    "During prediction, if a missing value is encountered for a feature, the classifier considers all possible states of the feature (i.e., present or missing) and calculates the likelihood of each class based on these possibilities.\n",
    "\n",
    "It's important to note that the treatment of missing values in Bernoulli Naive Bayes depends on the specific implementation and how missing values are handled in the preprocessing steps. In some cases, missing values may be imputed or replaced with a placeholder value before training the classifier, while in other cases, they may be treated as a distinct category during training and prediction. The choice of approach may depend on the characteristics of the data and the requirements of the classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e884fc-40fa-4831-9c8f-6294050d7c0f",
   "metadata": {},
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ef475e-d0a4-4741-9eac-42b77725e343",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification tasks. \n",
    "\n",
    "In Gaussian Naive Bayes, it is assumed that the continuous features follow a Gaussian (normal) distribution within each class. This assumption allows the classifier to estimate the mean and variance of each feature for each class. When classifying a new instance, the classifier computes the probability of the instance belonging to each class based on the Gaussian probability density function.\n",
    "\n",
    "For multi-class classification, the classifier calculates the probability of the instance belonging to each class individually and assigns the class with the highest probability as the predicted class for the instance.\n",
    "\n",
    "Gaussian Naive Bayes is particularly useful for classification tasks where the features are continuous and can be assumed to follow a Gaussian distribution within each class. It is commonly used in various applications, including text classification, medical diagnosis, and pattern recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d98887-c506-4198-9a90-e041ce3f85ac",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb5f24b-d151-4abf-9410-41100ba55453",
   "metadata": {},
   "source": [
    "To complete this assignment, you can follow these steps:\n",
    "\n",
    "1. **Data Preparation**:\n",
    "   - Download the \"Spambase Data Set\" from the provided link.\n",
    "   - Load the dataset into your Python environment.\n",
    "   - Split the dataset into features (X) and target variable (y).\n",
    "\n",
    "2. **Implementation**:\n",
    "   - Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using scikit-learn.\n",
    "   - Use 10-fold cross-validation to evaluate the performance of each classifier on the dataset.\n",
    "   - Calculate the accuracy, precision, recall, and F1 score for each classifier.\n",
    "\n",
    "3. **Results**:\n",
    "   - Report the performance metrics (accuracy, precision, recall, and F1 score) for each classifier.\n",
    "   - Discuss the results obtained from each classifier. Identify which variant of Naive Bayes performed the best and provide reasons for your observation.\n",
    "   - Identify any limitations or drawbacks of Naive Bayes classifiers observed during the evaluation.\n",
    "\n",
    "4. **Conclusion**:\n",
    "   - Summarize your findings from the evaluation of different Naive Bayes classifiers.\n",
    "   - Provide suggestions for future work, such as exploring other classification algorithms or improving feature selection techniques.\n",
    "\n",
    "Once you have completed these steps, you can organize your findings into a report format, including the data preparation steps, implementation details, results, discussion, and conclusion. Make sure to provide clear explanations and insights into the performance of each classifier and its implications for the task of spam email classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
